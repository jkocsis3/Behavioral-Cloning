{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Used to train the model.\n",
    "# Fix error with TF and Keras\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "import csv\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "from keras.layers import Convolution2D, Lambda\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "tf.python.control_flow_ops = tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load .csv file\n",
      "Images and labels loaded\n"
     ]
    }
   ],
   "source": [
    "cols = 32\n",
    "rows = 16\n",
    "# Open the data from the files\n",
    "def LoadImages(log, index, cols, rows):\n",
    "    # print(log[index].split('/')[1])\n",
    "    # need to swap the / to a \\ for Windows.\n",
    "    #print(log[index].split('/')[1])\n",
    "    img = Image.open(\"./IMG/%s\" % log[index].split('/')[1])\n",
    "    img = np.array(img)\n",
    "    return cv2.resize(img, (cols, rows))\n",
    "    # return cv2.resize(img, (cols, rows))\n",
    "\n",
    "    \n",
    "\n",
    "X = []\n",
    "y = []\n",
    "steeringAdjust = 0.3\n",
    "\n",
    "# open log file\n",
    "print(\"Load .csv file\")\n",
    "logs = []\n",
    "with open('driving_log.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        logs.append(row)\n",
    "\n",
    "for row in logs:\n",
    "    X.append(LoadImages(row, 0, cols, rows))\n",
    "    y.append(float(row[3]))\n",
    "    X.append(LoadImages(row, 1, cols, rows))\n",
    "    y.append(float(row[3]) + steeringAdjust)\n",
    "    X.append(LoadImages(row, 2, cols, rows))\n",
    "    y.append(float(row[3]) + steeringAdjust)\n",
    "\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "X = np.concatenate([X, X[:,:,::-1]])\n",
    "y = np.concatenate([y, -y])\n",
    "print(\"Images and labels loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train totals (46216, 16, 32, 3)\n",
      "Test totals (16, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "# Load the data into training and test sets\n",
    "xTrain = X[:-2000]\n",
    "yTrain = y[:-2000]\n",
    "xTest = X[-2000]\n",
    "yTest = y[-2000]\n",
    "print(\"Train totals\", xTrain.shape)\n",
    "print(\"Test totals\", xTest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: (array([ 0. ,  0.3,  0.3, ..., -0.3, -0.3, -0. ]),)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-07bc64597672>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;31m# One-Hot encode the labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlabel_binarizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0my_one_hot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel_binarizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myTrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\python3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\python3\\lib\\site-packages\\sklearn\\preprocessing\\label.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse_input_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\python3\\lib\\site-packages\\sklearn\\utils\\multiclass.py\u001b[0m in \u001b[0;36munique_labels\u001b[0;34m(*ys)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0m_unique_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FN_UNIQUE_LABELS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_unique_labels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unknown label type: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mrepr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mys_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_unique_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown label type: (array([ 0. ,  0.3,  0.3, ..., -0.3, -0.3, -0. ]),)"
     ]
    }
   ],
   "source": [
    "\n",
    "# One-Hot encode the labels\n",
    "label_binarizer = LabelBinarizer()\n",
    "y_one_hot = label_binarizer.fit_transform(yTrain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (36972, 16, 32, 3)\n",
      "Test (16, 32, 3)\n",
      "validation (9244, 16, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# split into train, test, validation\n",
    "xTrain, XValidation, yTrain, yValidation = train_test_split(xTrain, yTrain, test_size=0.2, random_state=0)\n",
    "print(\"Train\", xTrain.shape)\n",
    "print(\"Test\", xTest.shape)\n",
    "print(\"validation\", XValidation.shape)\n",
    "# shuffle the data\n",
    "xTrain, yTrain = shuffle(xTrain, yTrain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features normalized\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print('features normalized')\n",
    "\n",
    "\n",
    "def normalize_grayscale(image_data):\n",
    "    a = -0.5\n",
    "    b = 0.5\n",
    "    grayscale_min = 0\n",
    "    grayscale_max = 255\n",
    "    return a + (((image_data - grayscale_min) * (b - a)) / (grayscale_max - grayscale_min))\n",
    "\n",
    "\n",
    "X_normalized = normalize_grayscale(xTrain)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(32, 3, 3, input_shape=(16, 32, 3)))\n",
    "# print('conv 1', model.output_shape)\n",
    "# Max pooling will cut your shape in 1/2\n",
    "model.add(MaxPooling2D((2, 2), dim_ordering='tf'))\n",
    "# print('conv1 pool',model.output_shape)\n",
    "model.add(Dropout(0.5))\n",
    "# print('conv1 dropout',model.output_shape)\n",
    "model.add(Activation('relu'))\n",
    "# print('conv1 activation', model.output_shape)\n",
    "model.add(Flatten())\n",
    "# print('Flatten', model.output_shape)\n",
    "model.add(Dense(128))\n",
    "# print('FC1',model.output_shape)\n",
    "model.add(Activation('relu'))\n",
    "# print('FC1 Activation',model.output_shape)\n",
    "model.add(Dense(1))\n",
    "# print('FC2',model.output_shape)\n",
    "#model.add(Activation('softmax'))\n",
    "#print('FC 2 activation',model.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "Normalization (Lambda)           (None, 16, 32, 3)     0           lambda_input_5[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_5 (Convolution2D)  (None, 2, 32, 3)      34          Normalization[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_5 (MaxPooling2D)    (None, 2, 8, 1)       0           convolution2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 2, 8, 1)       0           maxpooling2d_5[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)              (None, 16)            0           dropout_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 1)             17          flatten_5[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 51\n",
      "____________________________________________________________________________________________________\n",
      "begin training\n",
      "Train on 29577 samples, validate on 7395 samples\n",
      "Epoch 1/40\n",
      "29577/29577 [==============================] - 6s - loss: 0.1645 - acc: 0.1662 - val_loss: 0.0800 - val_acc: 0.1824\n",
      "Epoch 2/40\n",
      "29577/29577 [==============================] - 7s - loss: 0.0802 - acc: 0.1817 - val_loss: 0.0740 - val_acc: 0.1824\n",
      "Epoch 3/40\n",
      "29577/29577 [==============================] - 6s - loss: 0.0739 - acc: 0.1817 - val_loss: 0.0689 - val_acc: 0.1824\n",
      "Epoch 4/40\n",
      "29577/29577 [==============================] - 5s - loss: 0.0704 - acc: 0.1817 - val_loss: 0.0650 - val_acc: 0.1824\n",
      "Epoch 5/40\n",
      "29577/29577 [==============================] - 5s - loss: 0.0670 - acc: 0.1817 - val_loss: 0.0601 - val_acc: 0.1824\n",
      "Epoch 6/40\n",
      "29577/29577 [==============================] - 5s - loss: 0.0633 - acc: 0.1817 - val_loss: 0.0559 - val_acc: 0.1824\n",
      "Epoch 7/40\n",
      "29577/29577 [==============================] - 5s - loss: 0.0619 - acc: 0.1817 - val_loss: 0.0542 - val_acc: 0.1824\n",
      "Epoch 8/40\n",
      "29577/29577 [==============================] - 5s - loss: 0.0613 - acc: 0.1817 - val_loss: 0.0524 - val_acc: 0.1824\n",
      "Epoch 9/40\n",
      "29577/29577 [==============================] - 4s - loss: 0.0611 - acc: 0.1817 - val_loss: 0.0525 - val_acc: 0.1824\n",
      "Epoch 10/40\n",
      "29577/29577 [==============================] - 4s - loss: 0.0612 - acc: 0.1817 - val_loss: 0.0528 - val_acc: 0.1824\n",
      "Epoch 11/40\n",
      "29577/29577 [==============================] - 5s - loss: 0.0606 - acc: 0.1817 - val_loss: 0.0516 - val_acc: 0.1824\n",
      "Epoch 12/40\n",
      "29577/29577 [==============================] - 6s - loss: 0.0606 - acc: 0.1817 - val_loss: 0.0523 - val_acc: 0.1824\n",
      "Epoch 13/40\n",
      "29577/29577 [==============================] - 5s - loss: 0.0608 - acc: 0.1817 - val_loss: 0.0520 - val_acc: 0.1824\n",
      "Epoch 14/40\n",
      "29577/29577 [==============================] - 5s - loss: 0.0605 - acc: 0.1817 - val_loss: 0.0523 - val_acc: 0.1824\n",
      "Epoch 15/40\n",
      "29577/29577 [==============================] - 5s - loss: 0.0608 - acc: 0.1817 - val_loss: 0.0525 - val_acc: 0.1824\n",
      "Epoch 16/40\n",
      "29577/29577 [==============================] - 5s - loss: 0.0603 - acc: 0.1817 - val_loss: 0.0519 - val_acc: 0.1824\n",
      "Epoch 17/40\n",
      "29577/29577 [==============================] - 5s - loss: 0.0599 - acc: 0.1817 - val_loss: 0.0513 - val_acc: 0.1824\n",
      "Epoch 18/40\n",
      "29577/29577 [==============================] - 6s - loss: 0.0583 - acc: 0.1817 - val_loss: 0.0498 - val_acc: 0.1824\n",
      "Epoch 19/40\n",
      "29577/29577 [==============================] - 6s - loss: 0.0575 - acc: 0.1817 - val_loss: 0.0496 - val_acc: 0.1824\n",
      "Epoch 20/40\n",
      "29577/29577 [==============================] - 6s - loss: 0.0574 - acc: 0.1817 - val_loss: 0.0493 - val_acc: 0.1824\n",
      "Epoch 21/40\n",
      "29577/29577 [==============================] - 5s - loss: 0.0572 - acc: 0.1817 - val_loss: 0.0492 - val_acc: 0.1824\n",
      "Epoch 22/40\n",
      "29577/29577 [==============================] - 5s - loss: 0.0571 - acc: 0.1817 - val_loss: 0.0488 - val_acc: 0.1824\n",
      "Epoch 23/40\n",
      "29577/29577 [==============================] - 5s - loss: 0.0574 - acc: 0.1817 - val_loss: 0.0488 - val_acc: 0.1824\n",
      "Epoch 24/40\n",
      "29577/29577 [==============================] - 5s - loss: 0.0572 - acc: 0.1817 - val_loss: 0.0494 - val_acc: 0.1824\n",
      "Epoch 25/40\n",
      "29577/29577 [==============================] - 4s - loss: 0.0570 - acc: 0.1817 - val_loss: 0.0499 - val_acc: 0.1824\n",
      "Epoch 26/40\n",
      "29577/29577 [==============================] - 5s - loss: 0.0569 - acc: 0.1817 - val_loss: 0.0488 - val_acc: 0.1824\n",
      "Epoch 27/40\n",
      "29577/29577 [==============================] - 5s - loss: 0.0566 - acc: 0.1817 - val_loss: 0.0494 - val_acc: 0.1824\n",
      "Epoch 28/40\n",
      "29577/29577 [==============================] - 5s - loss: 0.0574 - acc: 0.1817 - val_loss: 0.0492 - val_acc: 0.1824\n",
      "Epoch 29/40\n",
      "29577/29577 [==============================] - 5s - loss: 0.0572 - acc: 0.1817 - val_loss: 0.0493 - val_acc: 0.1824\n",
      "Epoch 30/40\n",
      "29577/29577 [==============================] - 5s - loss: 0.0570 - acc: 0.1817 - val_loss: 0.0492 - val_acc: 0.1824\n",
      "Epoch 31/40\n",
      "29577/29577 [==============================] - 4s - loss: 0.0568 - acc: 0.1817 - val_loss: 0.0492 - val_acc: 0.1824\n",
      "Epoch 32/40\n",
      "29577/29577 [==============================] - 4s - loss: 0.0575 - acc: 0.1817 - val_loss: 0.0496 - val_acc: 0.1824\n",
      "Epoch 33/40\n",
      "29577/29577 [==============================] - 5s - loss: 0.0570 - acc: 0.1816 - val_loss: 0.0483 - val_acc: 0.1824\n",
      "Epoch 34/40\n",
      "29577/29577 [==============================] - 4s - loss: 0.0570 - acc: 0.1817 - val_loss: 0.0491 - val_acc: 0.1824\n",
      "Epoch 35/40\n",
      "29577/29577 [==============================] - 4s - loss: 0.0569 - acc: 0.1817 - val_loss: 0.0495 - val_acc: 0.1824\n",
      "Epoch 36/40\n",
      "29577/29577 [==============================] - 5s - loss: 0.0570 - acc: 0.1817 - val_loss: 0.0493 - val_acc: 0.1824\n",
      "Epoch 37/40\n",
      "29577/29577 [==============================] - 5s - loss: 0.0570 - acc: 0.1817 - val_loss: 0.0493 - val_acc: 0.1824\n",
      "Epoch 38/40\n",
      "29577/29577 [==============================] - 5s - loss: 0.0571 - acc: 0.1817 - val_loss: 0.0490 - val_acc: 0.1824\n",
      "Epoch 39/40\n",
      "29577/29577 [==============================] - 5s - loss: 0.0572 - acc: 0.1817 - val_loss: 0.0487 - val_acc: 0.1824\n",
      "Epoch 40/40\n",
      "29577/29577 [==============================] - 5s - loss: 0.0571 - acc: 0.1817 - val_loss: 0.0496 - val_acc: 0.1824\n"
     ]
    }
   ],
   "source": [
    "model.summary() # print model summary\n",
    "model.compile('adam', 'mean_squared_error', ['accuracy'])\n",
    "print(\"begin training\")\n",
    "history = model.fit(xTrain, yTrain, nb_epoch=40, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load .csv file\n",
      "Load training dataset\n",
      "#########\n",
      "\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "Normalization (Lambda)           (None, 11, 32, 1)     0           lambda_input_4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 2, 32, 1)      24          Normalization[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_4 (MaxPooling2D)    (None, 2, 8, 1)       0           convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 2, 8, 1)       0           maxpooling2d_4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)              (None, 16)            0           dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 1)             17          flatten_4[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 41\n",
      "____________________________________________________________________________________________________\n",
      "Train on 43376 samples, validate on 4820 samples\n",
      "Epoch 1/40\n",
      "43376/43376 [==============================] - 2s - loss: 0.0792 - val_loss: 0.0698\n",
      "Epoch 2/40\n",
      "43376/43376 [==============================] - 2s - loss: 0.0639 - val_loss: 0.0498\n",
      "Epoch 3/40\n",
      "43376/43376 [==============================] - 2s - loss: 0.0488 - val_loss: 0.0349\n",
      "Epoch 4/40\n",
      "43376/43376 [==============================] - 2s - loss: 0.0399 - val_loss: 0.0285\n",
      "Epoch 5/40\n",
      "43376/43376 [==============================] - 2s - loss: 0.0362 - val_loss: 0.0275\n",
      "Epoch 6/40\n",
      "43376/43376 [==============================] - 2s - loss: 0.0348 - val_loss: 0.0263\n",
      "Epoch 7/40\n",
      "43376/43376 [==============================] - 1s - loss: 0.0351 - val_loss: 0.0267\n",
      "Epoch 8/40\n",
      "43376/43376 [==============================] - 1s - loss: 0.0344 - val_loss: 0.0257\n",
      "Epoch 9/40\n",
      "43376/43376 [==============================] - 2s - loss: 0.0345 - val_loss: 0.0261\n",
      "Epoch 10/40\n",
      "43376/43376 [==============================] - 2s - loss: 0.0343 - val_loss: 0.0254\n",
      "Epoch 11/40\n",
      "43376/43376 [==============================] - 2s - loss: 0.0340 - val_loss: 0.0260\n",
      "Epoch 12/40\n",
      "43376/43376 [==============================] - 2s - loss: 0.0341 - val_loss: 0.0253\n",
      "Epoch 13/40\n",
      "43376/43376 [==============================] - 2s - loss: 0.0342 - val_loss: 0.0254\n",
      "Epoch 14/40\n",
      "43376/43376 [==============================] - 2s - loss: 0.0336 - val_loss: 0.0258\n",
      "Epoch 15/40\n",
      "43376/43376 [==============================] - 2s - loss: 0.0339 - val_loss: 0.0259\n",
      "Epoch 16/40\n",
      "43376/43376 [==============================] - 2s - loss: 0.0342 - val_loss: 0.0256\n",
      "Epoch 17/40\n",
      "43376/43376 [==============================] - 1s - loss: 0.0341 - val_loss: 0.0255\n",
      "Epoch 18/40\n",
      "43376/43376 [==============================] - 2s - loss: 0.0338 - val_loss: 0.0255\n",
      "Epoch 19/40\n",
      "43376/43376 [==============================] - 1s - loss: 0.0339 - val_loss: 0.0256\n",
      "Epoch 20/40\n",
      "43376/43376 [==============================] - 2s - loss: 0.0338 - val_loss: 0.0255\n",
      "Epoch 21/40\n",
      "43376/43376 [==============================] - 2s - loss: 0.0340 - val_loss: 0.0251\n",
      "Epoch 22/40\n",
      "43376/43376 [==============================] - 2s - loss: 0.0341 - val_loss: 0.0246\n",
      "Epoch 23/40\n",
      "43376/43376 [==============================] - 2s - loss: 0.0338 - val_loss: 0.0259\n",
      "Epoch 24/40\n",
      "43376/43376 [==============================] - 1s - loss: 0.0339 - val_loss: 0.0251\n",
      "Epoch 25/40\n",
      "43376/43376 [==============================] - 1s - loss: 0.0337 - val_loss: 0.0251\n",
      "Epoch 26/40\n",
      "43376/43376 [==============================] - 1s - loss: 0.0339 - val_loss: 0.0259\n",
      "Epoch 27/40\n",
      "43376/43376 [==============================] - 1s - loss: 0.0339 - val_loss: 0.0254\n",
      "Epoch 28/40\n",
      "43376/43376 [==============================] - 1s - loss: 0.0338 - val_loss: 0.0251\n",
      "Epoch 29/40\n",
      "43376/43376 [==============================] - 1s - loss: 0.0337 - val_loss: 0.0256\n",
      "Epoch 30/40\n",
      "43376/43376 [==============================] - 2s - loss: 0.0338 - val_loss: 0.0258\n",
      "Epoch 31/40\n",
      "43376/43376 [==============================] - 2s - loss: 0.0338 - val_loss: 0.0255\n",
      "Epoch 32/40\n",
      "43376/43376 [==============================] - 1s - loss: 0.0339 - val_loss: 0.0255\n",
      "Epoch 33/40\n",
      "43376/43376 [==============================] - 1s - loss: 0.0342 - val_loss: 0.0260\n",
      "Epoch 34/40\n",
      "43376/43376 [==============================] - 1s - loss: 0.0337 - val_loss: 0.0263\n",
      "Epoch 35/40\n",
      "43376/43376 [==============================] - 1s - loss: 0.0338 - val_loss: 0.0253\n",
      "Epoch 36/40\n",
      "43376/43376 [==============================] - 1s - loss: 0.0340 - val_loss: 0.0262\n",
      "Epoch 37/40\n",
      "43376/43376 [==============================] - 1s - loss: 0.0339 - val_loss: 0.0253\n",
      "Epoch 38/40\n",
      "43376/43376 [==============================] - 1s - loss: 0.0339 - val_loss: 0.0257\n",
      "Epoch 39/40\n",
      "43376/43376 [==============================] - 1s - loss: 0.0336 - val_loss: 0.0255\n",
      "Epoch 40/40\n",
      "43376/43376 [==============================] - 1s - loss: 0.0336 - val_loss: 0.0250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2181c63ea90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import shuffle as sk_shuffle\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Lambda, Convolution2D,\\\n",
    "\tMaxPooling2D, Dropout, Flatten, Dense, Conv2D\n",
    "\n",
    "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-#\n",
    "\n",
    "# Parameters\n",
    "clean_sky = 50 # horizon pixel \n",
    "new_rows  = 11 # resized image size\n",
    "new_cols  = 32\n",
    "stering_theta = 0.3 # side cameras correction angle\n",
    "test_samples = 20   # use few images to test the CNN\n",
    "epochs = 40\n",
    "batch_size = 128\n",
    "\n",
    "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-#\n",
    "\n",
    "def loadAndPreprocessImage(log, idx, new_cols, new_rows):\n",
    "\timg = Image.open(\"IMG\\%s\"%log[idx].split('/')[-1]) # open .jpg\n",
    "\timg = np.array(img)\t\t\t\t\t\t\t     # convert to numpy array\n",
    "\n",
    "\t# extract saturation channel and crop out the sky\n",
    "\timg = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)[clean_sky:,:,1]  \n",
    "\treturn cv2.resize(img, (new_cols, new_rows))     # return resized image\n",
    "\n",
    "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-#\n",
    "\n",
    "# Load training data\n",
    "\n",
    "## open log file\n",
    "print(\"Load .csv file\")\n",
    "logs = []\n",
    "with open('driving_log.csv') as csvfile:\n",
    "\treader = csv.reader(csvfile, delimiter=',')\n",
    "\tfor row in reader:\n",
    "\t\tlogs.append(row)\n",
    "\n",
    "## preprocess images\n",
    "print(\"Load training dataset\")\n",
    "X = []\n",
    "y = []\n",
    "counter = 0\n",
    "for log in logs:\n",
    "\t# central camera\n",
    "\tX.append(loadAndPreprocessImage(log, 0, new_cols, new_rows))\n",
    "\ty.append(float(log[3]))\n",
    "\n",
    "\t# left camera\n",
    "\tX.append(loadAndPreprocessImage(log, 1, new_cols, new_rows))\n",
    "\ty.append(float(log[3]) + stering_theta)\n",
    "\n",
    "\t# right camera\n",
    "\tX.append(loadAndPreprocessImage(log, 2, new_cols, new_rows))\n",
    "\ty.append(float(log[3]) - stering_theta)\n",
    "\n",
    "\tif counter%1000 == 0:\n",
    "\t\tprint(\"#\", sep=' ', end='', flush=True)\n",
    "\tcounter += 1 \n",
    "print(\"\\n\")\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "## flip horizontally the images to augment the dataset\n",
    "X = np.concatenate([X, X[:,:,::-1]])\n",
    "y = np.concatenate([y, -y])\n",
    "\n",
    "## shuffle and split in training and test data\n",
    "X, y = sk_shuffle(X, y)\n",
    "\n",
    "X_train = X[:-test_samples,:,:,None]\n",
    "y_train = y[:-test_samples]\n",
    "X_test = X[-test_samples:,:,:,None]\n",
    "y_test = y[-test_samples:]\n",
    "\n",
    "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-#\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Lambda(lambda x: x/127.5 - 1.,\n",
    "\tinput_shape=(new_rows, new_cols, 1), name='Normalization'))\n",
    "model.add(Conv2D(2, 1, 1, border_mode='same', activation='relu'))\n",
    "model.add(MaxPooling2D((4, 4), (4, 4), 'same'))\n",
    "model.add(Dropout(0.3))\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))  # one neuron to rule them all\n",
    "\n",
    "model.summary() # print model summary\n",
    "\n",
    "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-#\n",
    "\n",
    "# Training\n",
    "model.compile(loss='mean_squared_error',optimizer='adam', ['accuracy'])\n",
    "\n",
    "# train the model using the 10% of the dataset for validation\n",
    "model.fit(X_train, y_train, batch_size=batch_size,\n",
    "\tnb_epoch=epochs, verbose=1, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
